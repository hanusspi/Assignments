{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db3cd635-0c48-43e7-8848-ecc7f323c0a4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5949d89106fc7cd3382e36048b3c017a",
     "grade": false,
     "grade_id": "cell-f4e4a1b4c96442c1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<img src=\"./img/DSME_logo.png\" alt=\"Scoring probabilities for direct shots [2]\" width=\"1000\">\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering (DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a730b584",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1cabc06bef719e9484391b3c5bac584f",
     "grade": false,
     "grade_id": "cell-ee4da126d6ed8633",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Bonus Point Assignment 1 - Task 1: Basketball Environment\n",
    "\n",
    "*Important: Each task includes a task description as well as the blocks “student answer”, and “checkpoints”. “Student answer” is for placing your answer, “checkpoint” helps you to debug. Only place your answer in fields marked for this purpose, and do not modify any of the cells. In case anything outside the answer fields has been modified, we recommend restarting the notebook.*\n",
    "\n",
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a06dece",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9629ab809fd45988c4f357714f9dcae",
     "grade": false,
     "grade_id": "cell-765820e95503a2d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.10.14)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env, spaces\n",
    "from gymnasium.envs.toy_text.utils import categorical_sample\n",
    "from gymnasium.error import DependencyNotInstalled\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "%matplotlib inline\n",
    "from IPython.display import Video\n",
    "from IPython.display import display\n",
    "from screeninfo import get_monitors\n",
    "from typing import Optional\n",
    "from render_util import render\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9ca76d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e6e9ca1efd06da41f091121614a3423",
     "grade": false,
     "grade_id": "cell-1a46f769dad4d238",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Task Description\n",
    "\n",
    "Environments are a key component of RL implementations. Thus, the purpose of this task is to give an understanding of how to implement environments. For this purpose, we turn our attention to basketball. Basketball is not only a popular team sport but also an active field for research. Many scientific works aim to help teams optimize their play, that is, to score as many points as possible. We are interested in the same question, for which we will give a brief overview of the rules in basketball. \n",
    "\n",
    "Abstracting the complex dynamics underlying the sport [1], the ball must be thrown into the basket. If the ball is thrown in front of the so-called three-point line, the team gets three points. If the ball is thrown from behind the line, which means that the player is closer to the basket, the team gets two points. Whenever the ball misses the basket, zero points are awarded. And, of course, should not leave the field.  \n",
    "\n",
    "In research, special attention is paid to the position from which the ball should be thrown. Simulation studies [2] provide insight into the issue. A two-dimensional analysis of the field results in varying score probabilities for direct hits depending on the field position, as shown below on the left. Additionally, data from professional games is collected and analyzed. According to [3], players throw from diverse positions, with the probability of hitting being highest when positioned close to the basket, with 63.2% in the 2018–2019 NBA season (shown in blue on the right).  \n",
    "\n",
    "<div style=\"clear: both;\">\n",
    "  <table style=\"float: left;\">\n",
    "    <tr>\n",
    "      <td> <img src=\"./img/goal_probabilities.png\" alt=\"Scoring probabilities for direct shots [2]\" width=\"415\"> </td>\n",
    "      <td> <img src=\"./img/goal_attempts.png\" alt=\"Shot Positions [3]\" width=\"320\"> </td>\n",
    "    </tr>\n",
    "  </table>\n",
    "  <p style=\"clear: both;\">\n",
    "  </p>\n",
    "</div>  \n",
    "\n",
    "Considering the results of the studies, a high degree of symmetry is noticeable. Using this symmetry, we establish a one-dimensional problem where the only variable determining the score probability is the distance to the basket. Thus, it is the state we observe. A discretization of the distances allows us to design a tabular environment whose essential dynamics are shown in the video below.\n",
    "\n",
    "<div style=\"clear: both;\">\n",
    "  <table style=\"float: left;\">\n",
    "    <tr>\n",
    "      <td> <img src=\"./img/basketball_example.gif\" alt=\"Example of the dynamics\" width=\"535\"> </td>\n",
    "      <td> <img src=\"./img/UML_BasketballEnv.png\" alt=\"UML Environment\" width=\"200\"> </td>\n",
    "    </tr>\n",
    "  </table>\n",
    "  <p style=\"clear: both;\">\n",
    "  </p>\n",
    "</div>  \n",
    "\n",
    "\n",
    " \n",
    "Creating this simplified basketball model is the main objective of this task. For this purpose, we provide the necessary methods for rendering. The environment will be implemented as a class, as shown above on the right. The class will be based on the toy-text environments from Gymnasium [4]. For more details on creating environments, we refer to the Gymnasium documentation ([4]).\n",
    "First, in task 1.1, we will program an MDP that reproduces the dynamics described above. Then, in task 1.2, we consider how an agent can interact with this environment. Additionally, we solve the environment using dynamic programming in task 1.3.\n",
    "\n",
    "**References**  \n",
    "[1] Brancazio, Peter J. \"Physics of basketball.\" American Journal of Physics 49.4 (1981): 356-365. [Link](https://aapt.scitation.org/doi/pdf/10.1119/1.12511)  \n",
    "[2] Silverberg, Larry M., Chau M. Tran, and Taylor M. Adams. \"Optimal targets for the bank shot in men's basketball.\" Journal of Quantitative Analysis in Sports 7.1 (2011). [Link](https://www.degruyter.com/document/doi/10.2202/1559-0410.1299/html)   \n",
    "[3] Wang, Feng, and Guohua Zheng. \"Examining positional difference in basketball players’ field goal accuracy using Bayesian Hierarchical Model.\" International Journal of Sports Science & Coaching 17.4 (2022): 848-859. [Link](https://journals.sagepub.com/doi/abs/10.1177/17479541221096772)  \n",
    "[4] Towers, Mark and Terry \"Gymnasium.\" Farama Foundation 2023. [Link](https://gymnasium.farama.org/index.html#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e831561e-3c5e-49ac-a2c8-017ef4470d69",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1b4c689c5679bf0e560325093d07072",
     "grade": false,
     "grade_id": "cell-f49112667fd90c13",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Task 1.1 (3 Points)\n",
    "\n",
    "Below, we show a drawing of the MDP of our Basketball environment.\n",
    "\n",
    "<img src=\"./img/basketball_mdp.png\" alt=\"Scoring probabilities for direct shots\" width=\"735\"> </td>\n",
    "\n",
    "We want to design the MDP with n states, counting from 0. State 0 represents the beginning of the field, while state n-1 represents the end. The length of the field is determined by the parameter `field_length`. Passing a value of 3 leads to three states 0, 1, 2 on the field. Furthermore, there are two terminal states; n, for when the agent scores, and n+1, for when the agent misses. Anywhere on the field, the agent can either throw the ball (action = 0) or move forward (action = 1). When the agent decides to move forward, reaching the next state has a probability of 100% and the agent receives a reward of 0. If the agent leaves the field via state n-1, the agent moves to n+1 and receives 0 reward. Furthermore, the agent can throw the ball in any state on the field. A throw attempt can either be successful, leading to state n, or unsuccessful, leading to state n+1. The probability of success increases linearly from a minimum value (`min_score_prob`) in the first state on the field, to a maximum value (`max_score_prob`) in the last state on the field. An unsuccessful attempt returns no reward, a successful attempt returns a reward of 3 if it was made in front of the line, and a reward of 2 if it was made behind the line. The position of the line is flexible and is passed to the environment as parameter `line_position`. If a value of 2 is passed, it means that state 2 is the first state behind the line.  \n",
    "\n",
    "The goal of this task is to transfer the dynamics described above into code. For this purpose, we use an implementation similar to the toy-text environments in Gymnasium. In these environments, the MDP is implemented in the `__init__` of the environment class as an attribute called `self.P`. This `self.P` is a dictionary that contains the available actions for all states, which are again structured as dictionaries. These dictionaries lead to possible transitions from the resulting state action pair. This results in the following form:\n",
    "\n",
    "&ensp; $ \\mathrm{self.P} = \\{$   \n",
    "&ensp; &ensp; $ s_0: \\{ a_0: [\\mathrm{transitions}(s_0, a_0)], a_1: [\\mathrm{transitions}(s_0, a_1)], a_2: [...], ... \\},$   \n",
    "&ensp; &ensp; $ s_1: \\{ a_0: [\\mathrm{transitions}(s_1, a_0)], a_1: [...], ... \\},$   \n",
    "&ensp; &ensp; $ s_2: \\{ a_0: [...], ... \\},$   \n",
    "&ensp; &ensp; $...$   \n",
    "&ensp; $\\}$   \n",
    "\n",
    "The transitions listed here are arrays of tuples, where each tuple represents a single possible transition in the form of\n",
    "\n",
    "&ensp; $[\\mathrm{transitions}(s_i, a_j)] = [(p(s'_0 | s_i, a_j), s'_0, r, t), (p(s'_1 | s_i, a_j), s'_1, r, t), (...)] $\n",
    "\n",
    "where we use the notation\n",
    "\n",
    "&ensp; $s_i$ = state i (int),   \n",
    "&ensp; $a_j$ = action j (int),   \n",
    "&ensp; $s'_k$ = successor states (int),   \n",
    "&ensp; $p(s'_k | s_i, a_j)$ = state transition probability (float),         \n",
    "&ensp; $r$ = reward (float), and     \n",
    "&ensp; $t$ = termination (boolean, True when the state leads to a terminating state).   \n",
    " \n",
    "Note that the reward and information about termination and successor states differ depending on the previous state and action; however, we neglected this dependency above for the sake of simple notation. By inserting all available information, we obtain the dynamics of our environment via `self.P`. Below, we give an example of `self.P` applied to our basketball environment. Here, parameters `min_score_prob=0.0`, `max_score_prob=0.9`, `field_length=6`, and `line_position=2` have been set. It follows the form:\n",
    "\n",
    "&ensp; `self.P`= $\\{$   \n",
    "<div style=\"clear: both;\">\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;font-weight:unset;padding:2px 5px;border-style:none;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:2px 5px;border-style:none;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg .tg-i0og{color:#000000}\n",
    "</style>\n",
    "<table class=\"tg\" style=\"margin-left: 20px; border-collapse: collapse; cellspacing: 1px;\">\n",
    "  <tr>\n",
    "    <td class=\"tg-i0og\" align=\"left\" > 0: {</td>\n",
    "    <td class=\"tg-031e\" align=\"left\" > 0: [(1, 1, 0, False)],</td>\n",
    "    <td class=\"tg-031e\" align=\"left\" > 1: [(0.0, 6, 3, True), (1.0, 7, 0, True)] </td>\n",
    "    <td class=\"tg-031e\" align=\"left\" > } </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-i0og\"> 1: { </td>\n",
    "    <td class=\"tg-031e\"> 0: [(1, 2, 0, False)], </td>\n",
    "    <td class=\"tg-031e\"> 1: [(0.18, 6, 3, True), (0.820001, 7, 0, True)] </td>\n",
    "    <td class=\"tg-031e\"> } </td>\n",
    "  </tr>  \n",
    "  <tr>\n",
    "    <td class=\"tg-i0og\"> 2: { </td>\n",
    "    <td class=\"tg-031e\"> 0: [(1, 3, 0, False)], </td>\n",
    "    <td class=\"tg-031e\"> 1: [(0.36, 6, 2, True), (0.64, 7, 0, True)] </td>\n",
    "    <td class=\"tg-031e\"> } </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-i0og\"> 3: { </td>\n",
    "    <td class=\"tg-031e\"> 0: [(1, 4, 0, False)], </td>\n",
    "    <td class=\"tg-031e\"> 1: [(0.54, 6, 2, True), (0.459999, 7, 0, True)] </td>\n",
    "    <td class=\"tg-031e\"> } </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-i0og\"> 4: { </td>\n",
    "    <td class=\"tg-031e\"> 0: [(1, 5, 0, False)], </td>\n",
    "    <td class=\"tg-031e\"> 1: [(0.72, 6, 2, True), (0.28, 7, 0, True)] </td>\n",
    "    <td class=\"tg-031e\"> } </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-i0og\"> 5: { </td>\n",
    "    <td class=\"tg-031e\"> 0: [(1, 7, 0, True)], </td>\n",
    "    <td class=\"tg-031e\"> 1: [(0.9, 6, 2, True), (0.099999, 7, 0, True)] </td>\n",
    "    <td class=\"tg-031e\"> } </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-i0og\"> 6: { </td>\n",
    "    <td class=\"tg-031e\"> 0: [(1, 6, 0, True)], </td>\n",
    "    <td class=\"tg-031e\"> 1: [(1, 6, 0, True)] </td>\n",
    "    <td class=\"tg-031e\"> } </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-i0og\"> 7:  {</td>\n",
    "    <td class=\"tg-031e\"> 0: [(1, 7, 0, True)] </td>\n",
    "    <td class=\"tg-031e\"> 1: [(1, 7, 0, True)] </td>\n",
    "    <td class=\"tg-031e\"> } </td>\n",
    "  </tr>\n",
    "</table>\n",
    "<span style='font-size:9px;'>   \n",
    "</div>   \n",
    "\n",
    "&ensp; $\\}$\n",
    "\n",
    "In the block below, we provide the initiation of the class `BasketballEnv`, including `__init__`, where `self.P` shall be implemented. We also provide all other important variables; thus, you only have to extend the existing code within the highlighted area in the block below. \n",
    "\n",
    "\n",
    "*Remark: In the MDP above, the terminal states have actions. In an MDP, the available actions per state can vary, so that in some implementations, no actions are assigned to the terminal states. Here, however, we assign the same actions to all states to simplify the implementation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed89f46",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9cfecdbab91b6b3c99c22933e49461ec",
     "grade": false,
     "grade_id": "cell-7af216cb2d1cab2d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Student Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69f4842-83e6-443e-abae-229812758b83",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "348165e86eaa4b1e2f24049953e33fc0",
     "grade": false,
     "grade_id": "cell-cd276516761bb726",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BasketballEnv(Env):\n",
    "    # Modes for rendering:\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\", None],\n",
    "        \"render_fps\": 4,\n",
    "    }\n",
    "    \n",
    "    def __init__(self, min_score_prob=0.0, max_score_prob=0.9, line_position = 3, field_length = 10, render_mode=None):\n",
    "        \"\"\" Initializes the environment and defines dynamics.\n",
    "        \n",
    "        Please DON'T change the names of the variables that are already defined in this method.\n",
    "        \"\"\"\n",
    "        self.render_mode = render_mode\n",
    "        self.render_time = 1 # one image per second\n",
    "        if self.render_mode is None:\n",
    "            gym.logger.warn(\n",
    "                \"You are calling render method without specifying any render mode. \"\n",
    "                \"You can specify the render_mode at initialization. \"\n",
    "            )\n",
    "        if self.render_mode != \"text\" and self.render_mode != None:\n",
    "            self.render_width = 130\n",
    "            self.window_size = ((field_length+1)*self.render_width, self.render_width)\n",
    "            self.cell_size = (self.render_width, self.render_width)\n",
    "            self.window_surface = None\n",
    "        self.state = 0 # starting state\n",
    "        self.laststate = None\n",
    "        self.field_length = field_length # max length of the field\n",
    "        self.line_position = line_position # position of the three point line\n",
    "        self.action_space = spaces.Discrete(2) # 0 = move, 1 = throw\n",
    "        self.observation_space = spaces.Discrete(self.field_length+2)\n",
    "        self.P = {}\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6e4646",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f361b9de3981af3b8d6d23a6fd755124",
     "grade": false,
     "grade_id": "cell-f82a365f21acc4a6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Checkpoint\n",
    "\n",
    "Below, we provide some tests for you to check whether your code works. Please consider using them for debugging purposes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124550c3-8615-442f-8526-94a4fc24c532",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ee0faf0728d50d801dcf533af061dac",
     "grade": false,
     "grade_id": "cell-0c2ab21a13fde350",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test environment for performing sanity checks\n",
    "gym.logger.set_level(40)\n",
    "field_length = 5\n",
    "line_position = 3\n",
    "test_env = BasketballEnv(field_length = field_length, line_position = line_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecffa1fa-80f4-4949-abfd-5792519e33a4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a53eb93c3cd33fa330ec554474843e10",
     "grade": false,
     "grade_id": "cell-a529d6e4ba2e9806",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Is the successor state correct?\n",
    "s = 0\n",
    "a = 0\n",
    "assert test_env.P[s][a][0][1] == s+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b65b3-a389-47cb-a76b-cf8c0e878f8a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0331430d3742bd7ccf996e7c1c996df",
     "grade": true,
     "grade_id": "cell-ff7d5a014949ee0d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used for grading. Do not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaa3ef7-6636-4572-a1de-37240d851c75",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7b92f930f6aab35ab98878f5a8a1dd3",
     "grade": false,
     "grade_id": "cell-a652c78746365e60",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do we reach the right states if we try to score?\n",
    "s = 3\n",
    "a = 1\n",
    "assert test_env.P[s][a][0][1] == 5 or test_env.P[s][a][0][1] == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6699ad4b-48d1-4c9d-9b08-ff7a3824896e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a99216fbf6d35869d633fedaeb1c7717",
     "grade": true,
     "grade_id": "cell-db5274da062e7c85",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used for grading. Do not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea4682f-5b0c-4661-91b1-10973752e2b7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80a699df27e1eef2c64268b234a7e57f",
     "grade": false,
     "grade_id": "cell-d090a7ce703a181c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Are the probabilities correct?\n",
    "s = 0\n",
    "a = 1\n",
    "assert test_env.P[s][a][0][0] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d72014-7bc8-44c9-9a9a-f6b7e56761ba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1035d74cdecdef20c4539fca5626cd99",
     "grade": true,
     "grade_id": "cell-8824c243fe8bb56d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used for grading. Do not change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88472ee1-77bb-4ebf-b989-77a5ad3598a5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7dcfc62f427b864fffbf8c7638f26133",
     "grade": false,
     "grade_id": "cell-06383d71e044411b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Task 1.2 (2 Points)\n",
    "\n",
    "By implementing the MDP in `self.P`, you have laid the foundation for using the environment to train agents. However, we still lack the necessary methods to allow interaction with the environment. Agents must be able to explore the environment using the method `step`. In addition, we need to be able to reset the environment after sampling, using the method `reset`. In the following, your task is to design these two methods. The notes below will provide guidance.\n",
    "\n",
    "`step`:\n",
    "- Initially, the current state should be cached in `self.laststate`.\n",
    "- For a given action, the method returns the new state, reward, and info about termination, as described in task 1.1, in exactly this order.\n",
    "- A new state is selected based on the current state and the given action using `self.P`. The current state can be retrieved from `self.state`. The selection follows the given probabilities.\n",
    "- The toy_text environments use a function called `categorical_sample`, which can simplify sampling. Information about how to use the function can be retrieved, for example, from the Frozen Lake implementation in the Gymnasium package.\n",
    "\n",
    "`reset`:\n",
    "- For each call, this method resets all relevant variables.\n",
    "- The method returns the initial state.\n",
    "- The method always resets to state 0.\n",
    "- The method resets `self.laststate` to `None`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae1f3a2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "af5020070d962d39c04dca3e5c7dfba8",
     "grade": false,
     "grade_id": "cell-1ec8d9f6fa661d53",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Student Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d14c343-3b4f-49ee-9e2f-c20c2b0ffb0e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7104497a5df56a2d8613be7eb90bd9b",
     "grade": false,
     "grade_id": "cell-97535539aef4a131",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reset(self):\n",
    "    \"\"\" Reset the environment.\n",
    "    \n",
    "    Args: \n",
    "        None\n",
    "    Returns: \n",
    "        Initial state\n",
    "    \"\"\"\n",
    "        \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27b71d0-ff57-4fef-b845-d759d4f71c33",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0de88eec3d8d835fc59149d9b58c3523",
     "grade": false,
     "grade_id": "cell-27cd2e723f14e758",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def step(self, action):\n",
    "    \"\"\" Take a step in the environment.\n",
    "    \n",
    "    Args:\n",
    "        action\n",
    "    Returns:\n",
    "        next state, reward, termination\n",
    "    \"\"\"\n",
    "    \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c71b08-0a29-43c0-a388-4b7f20e6bbe3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54352baa92cbb0d7ff973e23da9c5852",
     "grade": false,
     "grade_id": "cell-2e4cd40faefc5984",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adding the methods to the class of the environment\n",
    "\n",
    "setattr(BasketballEnv, 'reset', reset)\n",
    "setattr(BasketballEnv, 'step', step)\n",
    "setattr(BasketballEnv, 'render', render)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24ed3ef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d32cc3ec278f912da367feb6343c9b5d",
     "grade": false,
     "grade_id": "cell-bda30d4535448542",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a3f0de-6212-4fbd-822b-781bbebe4318",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e26cb52b3491eb2b73015b5a35cf6c9e",
     "grade": false,
     "grade_id": "cell-4778566fa1d7231e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "field_length = 10\n",
    "line_position = 5\n",
    "test_env = BasketballEnv(field_length = field_length, line_position = line_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6de5ab-4647-4173-bf7c-53a4cf2b6d4c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9143a19ddadc0f85838575e9341f5149",
     "grade": false,
     "grade_id": "cell-c4abb1bd50d189d1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Does the reset work?\n",
    "test_env.state = 8\n",
    "new_state = test_env.reset()\n",
    "assert test_env.state == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4b7ea3-2a5c-492d-8221-bdcd0ea2494f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8ce544b39badb63b255666ac2db95d6",
     "grade": true,
     "grade_id": "cell-044332bf1470f117",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used for grading. Do not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4215c8-9f64-47c0-8312-18c3df7bad30",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94a14743c79c7c21418539dfd7aea5fa",
     "grade": false,
     "grade_id": "cell-6893d5e35dc9fbf1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Does the step work?\n",
    "test_env.state = 5\n",
    "test_env.step(0) # moving forward\n",
    "assert test_env.state == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbf5884-4c81-4e39-8d29-7808ecad3686",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45806a1859223b6b7b2700f703c446e0",
     "grade": true,
     "grade_id": "cell-053f68c4e0ce772e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used for grading. Do not change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9f31ee-048e-4906-a3ad-0d32b0dfe683",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c156d90dc3d2e9f7805c82f783b6fde3",
     "grade": false,
     "grade_id": "cell-6cb4f904fff2be88",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## *Debug: Testing the Environment*\n",
    "\n",
    "By defining all the factors and dynamics, as well as adding methods for interacting with the environment, we have all the elements we need to use it. For better traceability, we import a visualization based on the `pygame` package. \n",
    "\n",
    "Further down, we make it possible to set a policy and observe the behavior of an agent in the environment based on it. Please consider using this to debug the previous code, as the environment is required for the next task. The policy contains a prescribed action for each state. Activating the block leads to the playback of a video that should reproduce the desired behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94457404-03eb-4164-a2da-2232b7ae3ead",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "343fa003ca978356bf190dafa7c6bfa0",
     "grade": false,
     "grade_id": "cell-44f1908d15ffbcaa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(env, policy, file, num_runs=5):\n",
    "    \"\"\" Evaluates the environment based on a policy.\n",
    "\n",
    "    Please use this method to debug your code for the environment.\n",
    "\n",
    "    Args:\n",
    "        env: Environment we want to use. \n",
    "        policy: Numpy array of shape (num_states, num_actions), for each state the array contains\n",
    "            the probabilities of entering the successor state based on the associated action. \n",
    "        file: File used for storing the video.\n",
    "        num_runs: Number of runs displayed.\n",
    "    \"\"\"\n",
    "    \n",
    "    frames = []  # collect rgb_image of agent env interaction\n",
    "    video_created = False\n",
    "    for _ in range(num_runs):\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        while not done:\n",
    "            action =  np.random.choice(np.flatnonzero(np.isclose(policy[obs], max(policy[obs]), rtol=0.0001)))\n",
    "            out = env.render()\n",
    "            frames.append(out)\n",
    "            obs, reward, done = env.step(action)\n",
    "            if done:\n",
    "                out = env.render()\n",
    "                frames.append(out)\n",
    "                \n",
    "    # create animation out of saved frames\n",
    "    if all(frame is not None for frame in frames):\n",
    "        fig = plt.figure(figsize=(10, 6))\n",
    "        plt.axis('off')\n",
    "        img = plt.imshow(frames[0][0])\n",
    "        def animate(index):\n",
    "            img.set_data(frames[index][0])\n",
    "            return [img]\n",
    "        anim = FuncAnimation(fig, animate, frames=len(frames), interval=20)\n",
    "        plt.close()\n",
    "        anim.save(file, writer=\"ffmpeg\", fps=2)\n",
    "        video_created = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279041d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For debug consider varying the parameters and changing the policy. Restarting the cell leads to video output\n",
    "\n",
    "env = BasketballEnv(min_score_prob = 0.0, max_score_prob = 0.8, line_position = 2, field_length = 10, render_mode = \"rgb_array\")\n",
    "policy = np.array([[1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0]]) # probabilies for actions per state\n",
    "video_file_1 = \"basketball.mp4\"\n",
    "evaluate(env, policy, video_file_1)\n",
    "Video(video_file_1, html_attributes=\"loop autoplay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230fa8c1-cd2f-458d-bbce-8ed2f79e687b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "53f3bad9672096e39123466f2b442766",
     "grade": false,
     "grade_id": "cell-208708f8e838378a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Task 1.3 (4 Points)\n",
    "\n",
    "We now have an environment that we can use to train an agent. Training requires a training method, for which we will use dynamic programming. For training the agent, we asked a student assistant to implement an `Agent` class capable of learning an optimal policy via policy iteration. Unfortunately, some errors occurred while the information was transferred from the textbook. Your task is to find the errors and fix the code. For this purpose, we show the algorithm for policy iteration below, as presented in [5].  \n",
    "\n",
    "<img src=\"./img/policy_iteration.png\" alt=\"Policy Iteration Algorithm [5]\" width=\"735\"> </td>\n",
    "\n",
    "Please correct the code below. Only some methods of the class `Agent` are flawed. The potentially flawed methods are commented out in the code provided below. Please copy *all* of them into the student answer block an correct them there to fix the Policy Iteration implementation.\n",
    "\n",
    "**References**   \n",
    "[5] Sutton, Richard S., and Andrew G. Barto, \"Dynamic Programming\", in *Reinforcement learning: An introduction*.  MIT press, 2018.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0486d5a4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd2f9fca8ce43740d846ef3dd8a983b9",
     "grade": false,
     "grade_id": "cell-19202a0c33c0524e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Student Answer\n",
    "Please enter your correct code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3f91c0-2105-4c19-9ac2-9964c5996fd2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97c464a457adc892fc82f8bb9fdb686f",
     "grade": false,
     "grade_id": "cell-3e67212e27d8d8e9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, env, gamma=0.9, update_threshold=1e-6):\n",
    "        \"\"\" Initializes the Agent.\n",
    "        \n",
    "        The agent takes properties of the environment and stores them for training.\n",
    "\n",
    "        Args:\n",
    "            env: Environment used for training.\n",
    "            gamma: Discount factor.\n",
    "            update_threshold: Stopping distance for updates of the value function.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.mdp = (env.unwrapped.P, env.observation_space.n, env.action_space.n)\n",
    "        self.update_threshold = update_threshold # stopping distance as criteria for stopping policy evaluation\n",
    "        self.state_value_fn = np.zeros(self.mdp[1]) # a table leading from state to value expectations\n",
    "        # Create random policy\n",
    "        self.policy = []\n",
    "        for state in range(self.mdp[1]):\n",
    "            random_entry = np.random.randint(0, 1)\n",
    "            self.policy.append([0 for _ in range(self.mdp[2])])\n",
    "            self.policy[state][random_entry] = 1\n",
    "        self.gamma = gamma # discount factor\n",
    "        self.iteration = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\" Resets the agent. \"\"\"\n",
    "        self.state_value_fn = np.zeros(self.mdp[1])\n",
    "        self.policy = []\n",
    "        for state in range(self.mdp[1]):\n",
    "            random_entry = np.random.randint(0, 1)\n",
    "            self.policy.append([0 for _ in range(self.mdp[2])])\n",
    "            self.policy[state][random_entry] = 1\n",
    "        self.iteration = 0\n",
    "\n",
    "    def get_greedy_action(self, state):\n",
    "        \"\"\" Choose an action based on the policy. \"\"\"\n",
    "        action = np.random.choice(np.flatnonzero(np.isclose(self.policy[state], max(self.policy[state]), rtol=0.01)))\n",
    "        return action\n",
    "    \n",
    "    def visualize(self):\n",
    "        \"\"\" Visualize the Q-function. \"\"\"\n",
    "        x_axis = 1\n",
    "        y_axis = self.mdp[1]-2 \n",
    "        vmin = min(self.state_value_fn)\n",
    "        vmax = max(self.state_value_fn)\n",
    "        X1 = np.reshape(self.state_value_fn[:-2], (x_axis, y_axis))\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        cmap = plt.colormaps[\"Blues_r\"]\n",
    "        cmap.set_under(\"black\")\n",
    "        img = ax.imshow(X1, interpolation=\"nearest\", vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(\"Values of the state value function on the field\")\n",
    "        for i in range(x_axis):\n",
    "            for j in range(y_axis):\n",
    "                ax.text(j, i, str(X1[i][j])[:4], fontsize=12, color='black', ha='center', va='center')\n",
    "        plt.show()\n",
    "        \n",
    "    def render_policy(self):\n",
    "        \"\"\" Print the current policy. \"\"\"\n",
    "        print('Policy of the agent:')\n",
    "        out = ' | '\n",
    "        render = out\n",
    "        for i in range(self.mdp[1]-2):\n",
    "            token = \"\"\n",
    "            if self.policy[i][0] > 0:   # move\n",
    "                token += \"Move\"\n",
    "            if self.policy[i][1] > 0:   # up\n",
    "                token += \"Throw\"\n",
    "            if len(token) > 5:\n",
    "                token = 'Move or Throw'\n",
    "            render += token + out\n",
    "        print(render) \n",
    "\n",
    "    # Below, the code seems to be flawed. We transferred the code into a comment, so you can copy it into your answer below\n",
    "    \n",
    "    \"\"\"\n",
    "    def train(self):\n",
    "        policy_stable = False\n",
    "        total_sweeps = 0\n",
    "        for i in range(100):\n",
    "            # Policy Evaluation\n",
    "            total_sweeps += self.policy_evaluation()\n",
    "            # Policy Improvement\n",
    "            policy_stable = self.policy_improvement()\n",
    "            self.iteration = 1\n",
    "        print('Sweeps required for convergence ', str(total_sweeps))\n",
    "        print('Iterations required for convergence ', str(self.iteration))\n",
    "\n",
    "    def policy_evaluation(self): \n",
    "        # in place version\n",
    "        sweeps = 0\n",
    "        stable = False\n",
    "        delta = 10\n",
    "        for i in range(1):\n",
    "            sweeps += 1\n",
    "            for state in range(self.mdp[1]):\n",
    "                old_state_value = self.state_value_fn[state]\n",
    "                new_state_value = 0\n",
    "                # sum over potential actions\n",
    "                for action in range(self.mdp[2]):\n",
    "                    new_state_value += self.get_policy_value(state, action)\n",
    "                self.state_value_fn[state] = new_state_value\n",
    "                delta = max(delta, np.abs(old_state_value - self.state_value_fn[state]))\n",
    "            if delta < self.update_threshold:\n",
    "                stable = True\n",
    "        return sweeps\n",
    "\n",
    "    def get_policy_value(self, state, action):\n",
    "        # Value expectation considering the policy\n",
    "        policy_value = 0\n",
    "        for transition in self.mdp[0][state][action]:\n",
    "            transition_prob = transition[0] # prob of next state\n",
    "            successor_state = transition[1] # value/name of next state\n",
    "            reward = transition[2] # reward of next state\n",
    "            policy_value += self.policy[state][action] * transition_prob * (reward + self.state_value_fn[successor_state])\n",
    "        return policy_value\n",
    "    \n",
    "    def get_action_value(self, state, action):\n",
    "        # Value expectation without considering the policy\n",
    "        action_value = 0\n",
    "        for transition in self.mdp[0][state][action]:\n",
    "            transition_prob = transition[0] # prob of next state\n",
    "            successor_state = transition[1] # value/name of next state\n",
    "            reward = transition[2] # reward of next state\n",
    "            action_value += transition_prob * (reward + self.state_value_fn[successor_state]) \n",
    "        return action_value\n",
    "\n",
    "    def policy_improvement(self):\n",
    "        policy_stable = True\n",
    "        current_policy = self.policy # Cache current policy\n",
    "        best_policy = []\n",
    "        for state in range(self.mdp[1]):\n",
    "            best_policy.append([0 for _ in range(self.mdp[2])])\n",
    "            # Calculate best possible policy based on current value function\n",
    "            action_values = []\n",
    "            for action in range(self.mdp[2]):\n",
    "                action_values.append(self.get_action_value(state, action))\n",
    "            best_actions = np.where(action_values == max(action_values))[0]\n",
    "            for index in best_actions:\n",
    "                best_policy[state][index] = 1\n",
    "            best_policy[state] = [best_policy[state][action] / len(best_actions)\n",
    "                                  for action in range(self.mdp[2])]\n",
    "            # If the current policy is not the best policy, update it\n",
    "            if not np.array_equal(current_policy[state], best_policy[state]):\n",
    "                policy_stable = False\n",
    "                self.policy[state] = best_policy[state]\n",
    "        return policy_stable\n",
    "    \"\"\"\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe99899f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "255bcec6a6d05088f7f55b92982f79b6",
     "grade": false,
     "grade_id": "cell-735b80d76a153ba2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f04fda-63cb-49f2-aedd-2c6e46f8dfc8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b70fdd0ec2ce6b25f419699bd34ff99",
     "grade": false,
     "grade_id": "cell-dd577d2e1a45dc9e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "field_length = 12\n",
    "line_position = 6\n",
    "test_env = BasketballEnv(field_length = field_length, line_position = line_position)\n",
    "test_agent = Agent(test_env, gamma=0.9, update_threshold=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554931dd-636f-4c68-8ae6-71a42233fd4a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78d15d80863a3b6ca5038df1b1b70f88",
     "grade": false,
     "grade_id": "cell-fa5c30f5c4a3e308",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Does the visualization work?\n",
    "test_agent.state_value_fn = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4]\n",
    "test_agent.visualize()\n",
    "test_agent.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61db94e2-4f91-40bd-9965-15bac36c2eaf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28bc1d18c69ad2cae756cd4694b1f6b9",
     "grade": true,
     "grade_id": "cell-13a43b7ba9b518b6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used for grading. Do not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a9e40a-afeb-472c-86bf-31bb9ed8bbe5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cedb61420887b609e1a86037128678ce",
     "grade": false,
     "grade_id": "cell-efe818bdc1af5f19",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Can we evaluate a policy?\n",
    "test_agent.policy = [[0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0]]\n",
    "test_agent.policy_evaluation()\n",
    "test_agent.visualize()\n",
    "test_agent.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0728b7f7-2d8b-4c57-a519-9f7cd7886eba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f1e70c65846a4898f4fa604de38a8e2a",
     "grade": true,
     "grade_id": "cell-d4f09de8c1dee0e8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used for grading. Do not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d426adf-0929-4d55-a9a2-40b8be4649cc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "402f2ca3c2c6128917faef72ded2ab2a",
     "grade": false,
     "grade_id": "cell-2156dcb444a746c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Does the training work?\n",
    "test_agent.train()\n",
    "test_agent.visualize()\n",
    "test_agent.render_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed137d92-8699-4e42-86da-435fb5ea5ec7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed3af8d4c4a66cd9db8a51d51c702c6f",
     "grade": true,
     "grade_id": "cell-9a00b76b48ab4c68",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used for grading. Do not change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0028ea4-13ec-415e-86cd-2d570c0c7452",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e7e8e395ab6fdd90d408469612e1a6c",
     "grade": false,
     "grade_id": "cell-7d11c97fb0a093b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Finally: Training the Agent (1 Point)\n",
    "\n",
    "We verify that the environment can be used to train an agent with policy iteration to generate an optimal policy. For this, we create an environment, train an agent, and show the results. An optimal policy should be displayed in the video below. In case errors occur, please check your answers up to this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799908af-eaaa-4959-a1f7-f93862c77687",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e7b95c59b3eddcfb6143e48e9b2ebd2",
     "grade": false,
     "grade_id": "cell-58698d8bfd1ab72c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = BasketballEnv(min_score_prob = 0.1, max_score_prob = 0.95, line_position = 2, field_length = 8, render_mode = \"rgb_array\")\n",
    "test_agent = Agent(env, gamma = 0.99)\n",
    "test_agent.train()\n",
    "\n",
    "video_file_2 = \"basketball_training.mp4\"\n",
    "evaluate(env, test_agent.policy, video_file_2)\n",
    "Video(video_file_2, html_attributes=\"loop autoplay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094373c2-f4ed-4c4f-adaf-72307e012b4c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b6c592fed32bdd09a0df2e2bb7d31cba",
     "grade": true,
     "grade_id": "cell-003257b5a61ba894",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is the end of task 1, please proceed with task 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
