{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "76b21394bd8ea88eafa7a1160f63dad5",
     "grade": false,
     "grade_id": "cell-bc3e02286445e490",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<img src=\"./img/DSME_logo.png\" alt=\"Scoring probabilities for direct shots [2]\" width=\"1000\">\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering (DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2639a46ce6a000f5f7dc00771116f779",
     "grade": false,
     "grade_id": "cell-26800b5858579af5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Bonus Point Assignment 1 - Task 3: Off-Policy MC with Weighted Importance Sampling\n",
    "\n",
    "*Important: Place your answer in fields marked for this purpose and do not modify any of the cells. In case anything outside the answer fields has been modified, we recommend restarting the notebook.*\n",
    "\n",
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af035232977c1acecea3a030fc0d5cf2",
     "grade": false,
     "grade_id": "cell-c89db129b4e8cbe2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import custom_envs\n",
    "from render_util import plot_action_value\n",
    "from matplotlib.animation import FuncAnimation\n",
    "%matplotlib inline\n",
    "from IPython.display import Video\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ecdcf00ec7eb252c7b9506ebcfb5bf2",
     "grade": false,
     "grade_id": "cell-a6bfb6b4f7672754",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Task Description\n",
    "\n",
    "With Q-learning, we've seen an off-policy TD method. Off-policy methods can be helpful because they allow for the reuse of data. But how can we implement this concept for Monte Carlo (MC) methods, where we consider complete rollouts rather than individual transitions? First, it requires the introduction of two different policies:\n",
    "* *behavior policy* ($b$), which interacts with the environment.\n",
    "* *target policy* ($\\pi$), that we aim to optimize.\n",
    "\n",
    "To optimize $\\pi$ using $b$, we must ensure that actions taken under $\\pi$ are at least occasionally taken under $b$. We refer to this as the assumption of coverage. Based on this assumption, we can update $\\pi$ using rollouts from $b$, via importance sampling, as introduced in the lecture. Below, we summarize the key features of importance sampling.\n",
    "\n",
    "Importance sampling is a method that allows us to compute the value function of a policy using samples from another policy. For this purpose, we require the relative probability of trajectories from the target policy occurring under the behavior policy. We call this the *importance sampling ratio*, calculated as \n",
    "\n",
    "&ensp; $\\rho_{t:T-1} = \\prod^{T-1}_{k=t} \\frac{\\pi(A_k \\mid S_k)}{b(A_k \\mid S_k)}$, \n",
    "\n",
    "with $A_k$ being the action taken in time step $k$ and $S_k$ being the according state. As the index of $\\rho_{t:T-1}$ shows, we consider trajectories from time step $t$ to step $T-1$, where $T$ is the time step of termination (hence there is no sampling of an action in $T$). Based on the importance sampling ratio, we can estimate $V(s)$ of $\\pi$ as the average importance weighted return \n",
    "\n",
    "&ensp; $V(s) = \\frac{\\sum _{t \\in \\tau (s) } \\rho_{t:T(t)-1} G_t}{\\sum _{t \\in \\tau (s) } \\rho_{t:T(t)-1}}$,\n",
    "\n",
    "where $G_t$ is the return following time step $t$ and $\\tau (s)$ is the set of time steps in which state $s$ is visited (either determined via first-visit or every-visit methods). Note that we consider multiple episodes for the estimation of the value function. Thus, it is helpful to number $t$ consecutively, which means that the start of a new episode takes place in $t=T+1$. Consequently, we have several terminations of episodes, which is why we introduce $T(t)$, whereby the termination always refers to the episode we are looking at. The type of importance introduced above is known as *weighted* importance sampling. The name refers to the fact that dividing by the sum of the importance sampling ratios bounds the estimates of the value function, which can lead to better performance compared to *ordinary* importance sampling [1].\n",
    "\n",
    "By using importance sampling, we are now capable of implementing an off-policy Monte Carlo reinforcement learning algorithm that learns a control policy. An example of such an algorithm, as proposed by [1], is given below on the left.\n",
    "\n",
    "<div style=\"clear: both;\">\n",
    "  <table style=\"float: left;\">\n",
    "    <tr>\n",
    "      <td> <img src=\"./img/off-policy_MC_importance_sampling.png\" alt=\"Off-Policy MC Control [1]\" width=\"505\"> </td>\n",
    "      <td> <img src=\"./img/UML_MC.png\" alt=\"UML Agent\" width=\"230\"> </td>\n",
    "    </tr>\n",
    "  </table>\n",
    "  <p style=\"clear: both;\">\n",
    "  </p>\n",
    "</div>  \n",
    "\n",
    "Your task is to implement the algorithm as part of a class `Agent`. Note that we call the behavior policy `self.behavior_policy` instead of $b$ within our algorithm. A diagram of the `Agent` class is shown above on the right.\n",
    "\n",
    "We define the agent step by step, aiming to train an agent in the “FrozenLake” environment [2]. We start in task 3.1 by defining the policies, followed by designing the necessary methods for interacting with environments in task 3.2. Finally, we focus on the learning algorithm in task 3.3. \n",
    "\n",
    "[1] Sutton, Richard S., and Andrew G. Barto, \"Monte Carlo Methods\", in *Reinforcement learning: An introduction*. MIT press, 2018.  \n",
    "[2] Towers, Mark and Terry \"Gymnasium.\" Farama Foundation 2023. [Link](https://gymnasium.farama.org/index.html#)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "398372992e3954790d5d99d06930eb5f",
     "grade": false,
     "grade_id": "cell-605c71663d4572bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Task 3.1 (1 Point)\n",
    "\n",
    "Below, we provide the basic structure of the class `Agent`, lacking only a part of the implementation of the target policy. Your task is to complete the method `make_target_policy`, which shall include a nested function `policy(obs)` to automatically handle policy updates. The assignment of probabilities should follow the following rules:\n",
    "\n",
    "* When action values differ by more than rtol=0.01, the action with the highest value gets a probability of 1.0.\n",
    "* When differences between some values are below rtol, each action gets an equal probability.\n",
    "\n",
    "*Hint: If the purpose behind the nested function is not clear, you can consider the structure of `make_random_policy` for guidance. For determining the probabilities it is worth studying the previous tasks for the assignment of probabilities.* \n",
    "\n",
    "### Student Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b6f77de2d07036de617ecb1aa63b628",
     "grade": false,
     "grade_id": "cell-86a8eaa3a8186f52",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env, gamma=0.9):\n",
    "        \"\"\" Initializes the environment and defines dynamics.\n",
    "        \n",
    "        Please DON'T change the names of the variables that are already defined in this method.\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.action_value_fn = np.zeros((self.env.observation_space.n, self.env.action_space.n))\n",
    "        self.C = np.zeros((self.env.observation_space.n, self.env.action_space.n)) # Cumulative sum of ratios\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Generate Policies\n",
    "        def make_random_policy():\n",
    "            def policy(obs):\n",
    "                b = np.ones(self.env.action_space.n, dtype=float) / self.env.action_space.n\n",
    "                return b\n",
    "            return policy\n",
    "        self.behavior_policy = make_random_policy()\n",
    "\n",
    "        def make_target_policy():\n",
    "            # Updating continously based on the action value function\n",
    "            def policy(obs):\n",
    "                pi = np.zeros(self.env.action_space.n, dtype=float)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "                return pi\n",
    "            return policy\n",
    "        self.target_policy = make_target_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "690215deb0eb09c1965af06f2cbee156",
     "grade": false,
     "grade_id": "cell-d4c5aa9387dbb01c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3680e1fd596cef8572e4983f66c8ab3",
     "grade": false,
     "grade_id": "cell-00621a96d299f9ef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do the policies return values for all the actions?\n",
    "map = [\"SFFH\", \"FFFH\", \"HFFH\", \"HFFG\"]\n",
    "test_env = gym.make('CustomFrozenLake-v1', render_mode=None, desc=map, is_slippery=False) \n",
    "test_agent = Agent(test_env)\n",
    "assert len(test_agent.target_policy(0)) == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9d04d86c4f3195580925d32a72803ce7",
     "grade": true,
     "grade_id": "cell-925c4ba17d3c0af1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used for grading. Do not change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "02e316ba2b4284d1578e25c69b56f0bf",
     "grade": false,
     "grade_id": "cell-5757ad1b3bf58ec7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Task 3.2 (1 Point)\n",
    "\n",
    "Next, we need a method that selects actions based on the policies defined in the task above. Your task is to implement this method called `get_action`. It receives an observation and a policy and returns an action based on the policy. Selecting the action shall be based on `np.random.choice(A, p=(...))`, where `A` is an array including all actions and `p` is an array that assigns probabilities to these actions, i.e., our policy based on the observation.\n",
    "\n",
    "*Remark: The ability to select a policy is crucial for sampling different policies.*\n",
    "\n",
    "### Student Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e64f0267cdf988522dfe8f25be8aa236",
     "grade": false,
     "grade_id": "cell-bacb7665fbffd715",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c08be2fcb8c532e9dd1859cccdb20ea",
     "grade": false,
     "grade_id": "cell-c4483fedc6e6c852",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "setattr(Agent, 'get_action', get_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "183b9c71587a19d4eb3108a165b9fb45",
     "grade": false,
     "grade_id": "cell-98eaa114936c0592",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c53f9becc52374b7b1647740be9876b7",
     "grade": false,
     "grade_id": "cell-cf8d20ddad9d73d5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Can we pick an action?\n",
    "map = [\"SFFH\", \"FFFH\", \"HFFH\", \"HFFG\"]\n",
    "test_env = gym.make('CustomFrozenLake-v1', render_mode=None, desc=map, is_slippery=False) \n",
    "test_agent = Agent(test_env)\n",
    "test_agent.action_value_fn[0] = [1, 0, 0, 0]\n",
    "assert test_agent.get_action(test_agent.target_policy, 0) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d32638665879acb7b5e95842e1a78059",
     "grade": true,
     "grade_id": "cell-23e277fa723a3a1d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used for grading. Do not change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50a8db31ff24abeb7fb87cbeaed7e0f4",
     "grade": false,
     "grade_id": "cell-b89b432f6d815052",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Task 3.3 (2 Points)\n",
    "\n",
    "The update scheme of the action value function is implemented in the `train` function of the agent. We provide most of it below, lacking only the updates of the return and the importance sampling ratios. Your task is to identify and add the missing methods for the updates in the code below. The comments in the code are intended to help with questions about the meaning of variables.\n",
    "\n",
    "*Remark: The training consists of two distinct phases: 1) generating an episode, 2) performing updates based on the episode. To generate the episode, we use the behavior policy. For updating the Q-function, the total discounted return $G$ and the importance sampling ratios are calculated by going through the episode from the back, as this is particularly efficient. By updating the Q-function, we're implicitly updating the target policy.*\n",
    "\n",
    "### Student Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "731913d48884cc8174b288a11f4d5ae0",
     "grade": false,
     "grade_id": "cell-63ef3c45fe4723c3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(self, num_episodes, episode_max_duration=100):\n",
    "    \"\"\" Trains the Agent using the given algorithm.\n",
    "\n",
    "    Inputs:\n",
    "        num_episodes: Number of episodes for which the training lasts.\n",
    "        episdoe_max_duration: Maximal duration of an episode. Once the number of steps reaches the threshold,\n",
    "            the episode is terminated.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Run through episodes sampled to improve policy incrementally\n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        # Generate an episode using the behavior policy [(obs, action, reward), (...), ...]\n",
    "        episode = []\n",
    "        obs, info = env.reset()\n",
    "        for t in range(episode_max_duration):\n",
    "            action = self.get_action(self.behavior_policy, obs)\n",
    "            next_obs, reward, done, truncated, info = env.step(action)\n",
    "            episode.append((obs, action, reward))\n",
    "            if done:\n",
    "                break\n",
    "            obs = next_obs\n",
    "        episode = np.array(episode)\n",
    "        episode_duration = len(episode[:,:1])\n",
    "        # Calculate returns and update the policy using weighted importance sampling from the back to save resources\n",
    "        G = 0.0                                           # Sum of discounted returns\n",
    "        W = 1.0                                           # Ratios\n",
    "        for i in range(episode_duration - 1, -1, -1):\n",
    "            obs = int(episode[i][0])\n",
    "            action = int(episode[i][1])\n",
    "            reward = episode[i][2]\n",
    "            # Update the return\n",
    "            G = update_return(self.gamma, G, reward)\n",
    "            # Sum up all the sampling ratios\n",
    "            self.C[obs][action] += W \n",
    "            # Update the action value function (implicitly updates the target policy as well)\n",
    "            self.action_value_fn[obs][action] += (W / self.C[obs][action]) * (G - self.action_value_fn[obs][action])\n",
    "            # Update the current sampling ratio\n",
    "            W = update_W(W, self.target_policy(obs)[action], self.behavior_policy(obs)[action]) \n",
    "            if W == 0:\n",
    "                break\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7ae2fa81e2382e04b9e538a4344c73f",
     "grade": false,
     "grade_id": "cell-19d75a9161310e63",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "setattr(Agent, 'update_return', update_return)\n",
    "setattr(Agent, 'update_W', update_W)\n",
    "setattr(Agent, 'train', train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da390e9031e4494015e7780a88c3b9a2",
     "grade": false,
     "grade_id": "cell-e24f430ae9feda7f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4dbe60812f656e65eee79ba28b203539",
     "grade": false,
     "grade_id": "cell-b5d8fa9977dd257e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Can we calculate returns?\n",
    "\n",
    "assert update_return(1.0, 1.0, 1.0) != 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f24855689300a83a24a42c0e5d8bbcd9",
     "grade": true,
     "grade_id": "cell-8b8f563e1cd09027",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used for grading. Do not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c5774a1bcb03889608b97703620dcdf",
     "grade": false,
     "grade_id": "cell-81b884d9b19af97b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Can we calculate updates for roh?\n",
    "\n",
    "assert update_W(1.0, 0.75, 0.75) == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "46919ae054223d51cd9169972c0fa159",
     "grade": true,
     "grade_id": "cell-92d1ebbe7c28f84f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used for grading. Do not change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "21bf8ac8e375f0655b22a8c45d6c058c",
     "grade": false,
     "grade_id": "cell-96c0efcff900e000",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Finally: Training the Agent (1 Point)\n",
    "\n",
    "Lastly, we verify that our agent can train in the selected environment. If everything has been implemented according to the task, you should see a video of the agent solving the task successfully. This will also provide feedback on whether your code is working successfully.\n",
    "\n",
    "*Note: Completing the task should clarify the following: We see multiple paths to the goal on the plot. This is not surprising since there are several equally valid paths. If we compare this with other agents, we see the explorative behavior resulting from the design of our behavior policy. However, there are various other ways to implement exploration behavior during training.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b300353507c768934494345b205649fa",
     "grade": false,
     "grade_id": "cell-605985c3a87a0800",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(self, env, file, num_runs=5):\n",
    "    \"\"\" Evaluates the agent in the environment.\n",
    "\n",
    "    Args:\n",
    "        env: Environment we want to use. \n",
    "        file: File used for storing the video.\n",
    "        num_runs: Number of runs displayed\n",
    "    \"\"\"\n",
    "    frames = []  # collect rgb_image of agent env interaction\n",
    "    video_created = False\n",
    "    for _ in range(num_runs):\n",
    "        done = False\n",
    "        obs, info = env.reset()\n",
    "        out = env.render()\n",
    "        frames.append(out)\n",
    "        while not done:\n",
    "            action = self.get_action(self.target_policy, obs)\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            out = env.render()\n",
    "            frames.append(out)\n",
    "    # create animation out of saved frames\n",
    "    if all(frame is not None for frame in frames):\n",
    "        fig = plt.figure(figsize=(10, 6))\n",
    "        plt.axis('off')\n",
    "        img = plt.imshow(frames[0])\n",
    "        def animate(index):\n",
    "            img.set_data(frames[index])\n",
    "            return [img]\n",
    "        anim = FuncAnimation(fig, animate, frames=len(frames), interval=20)\n",
    "        plt.close()\n",
    "        anim.save(file, writer=\"ffmpeg\", fps=5) \n",
    "    return\n",
    "\n",
    "setattr(Agent, 'evaluate', evaluate)\n",
    "setattr(Agent, 'plot_action_value', plot_action_value)\n",
    "\n",
    "map = [\"SFFH\", \"FFFH\", \"HFFH\", \"HFFG\"]\n",
    "env = gym.make('CustomFrozenLake-v1', render_mode='rgb_array', desc=map, is_slippery=False) \n",
    "env.reset()\n",
    "agent = Agent(env, gamma=0.9)\n",
    "agent.train(num_episodes=5000)\n",
    "agent.plot_action_value()\n",
    "video = \"final_run.mp4\"\n",
    "agent.evaluate(env, video, num_runs=5)\n",
    "Video(video, html_attributes=\"loop autoplay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd905bbb3b7cfd6a317252f88271cebb",
     "grade": true,
     "grade_id": "cell-23be048578fd6787",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is the end of task 3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllbc-bpa",
   "language": "python",
   "name": "rllbc-bpa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
