{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57f113e0-5955-4762-953d-14a094e245ee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8bf20328916dda348fc1fc6472ef4efc",
     "grade": false,
     "grade_id": "cell-0dfa73d3d73c8955",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<img src=\"./img/DSME_logo.png\" alt=\"Scoring probabilities for direct shots [2]\" width=\"1000\">\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering (DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3205ff3-74fb-463d-b326-3ceea1a0d5d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dedd4c89863a61f1425b96c768665c88",
     "grade": false,
     "grade_id": "cell-93b67c9772b15f9c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Bonus Point Assignment 1 - Task 2: Double-Q Learning\n",
    "\n",
    "*Important: Place your answer in fields marked for this purpose and do not modify any of the cells. In case anything outside the answer fields has been modified, we recommend restarting the notebook.*\n",
    "\n",
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2655121-84fa-4928-b4b4-4335e197f30a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b3019c3fe244fead75a8130e168dc154",
     "grade": false,
     "grade_id": "cell-205096309dce77e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import custom_envs\n",
    "from render_util import visualize, plot_action_value\n",
    "from matplotlib.animation import FuncAnimation\n",
    "%matplotlib inline\n",
    "from IPython.display import Video\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f69e5-d5ea-4420-b583-e0f25a22dc6f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b4f0850b5d9e314ed3c45250f5b48f2",
     "grade": false,
     "grade_id": "cell-62d865961bf7f0ec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Task Description\n",
    "\n",
    "Often we don't know the environment dynamics a-priori, which hinders us from applying dynamic programming methods. Here, reinforcement learning approaches like Q-learning can help [1]. As we know from lecture, Q-learning gradually adapts its estimate of the action value function based on data observed using temporal difference (TD) learning. The off-policy nature of Q-learning results in a TD target formulated with respect to the greedy target policy, making it dependent on the highest action value for the next state [2]. In noisy setups, e.g. when the reward emitted by the environment is noisy or function approximation is used to represent the Q-function, this can lead to overestimation bias of the Q-function [3].\n",
    "\n",
    "Double Q-learning mitigates this issue by learning two Q-functions, seperating the process of action selection and it's evaluation in the computation of the TD target [4]. This way, overestimation occurs less, and better performance can be demonstrated in some scenarios. An example is shown below.  \n",
    "\n",
    "<img src=\"./img/maximization_bias.png\" alt=\"Effect of the Maximization Bias in Q-Learning [5]\" width=\"735\">   \n",
    "\n",
    "The authors of [5] show that for a simple MDP, as shown in the upper-right corner, double Q-learning converges to the optimal solution much faster. For further details on the example, we refer to [5]. The performance was achieved by using an algorithm following the description presented below on the left. Here, the policy is chosen randomly. However, a different logic can be used for the selection of the respective Q-function. Nevertheless, we will follow the proposed logic in [5].    \n",
    "\n",
    "<div style=\"clear: both;\">\n",
    "  <table style=\"float: left;\">\n",
    "    <tr>\n",
    "      <td> <img src=\"./img/double_q_learning.png\" alt=\"Algorithm for Double Q-Learning [5]\" width=\"535\"> </td>\n",
    "      <td> <img src=\"./img/UML_DoubleQ.png\" alt=\"UML Agent\" width=\"200\"> </td>\n",
    "    </tr>\n",
    "  </table>\n",
    "  <p style=\"clear: both;\">\n",
    "  </p>\n",
    "</div>  \n",
    "\n",
    "We can observe many similarities to Q-learning. For an easier understanding, we use the term “learning_rate” instead of &alpha; in the course of this task.\n",
    "Your task is to implement the algorithm above in a class `Agent`, as shown in the diagram above on the right. We will test the ability of the algorithm to solve reinforcement learning tasks in the “FrozenLake” environment [6], with some modifications. In particular, we prevent the agent from “sliding” around by setting the probability of moving in the intended direction to 100%. The goal is for the elve to move to the package without entering an ice hole. For an example of the environment, see below. \n",
    "\n",
    "<img src=\"./img/frozen_lake.gif\" alt=\"Example of Frozen_Lake [6]\" width=\"380\">   \n",
    "\n",
    "First, we design the Q-functions in task 2.1, followed by the policies in task 2.2 and the training method in task 2.3. Finally, in task 2.4, we create a method to train and evaluate the agent in an environment.  \n",
    "\n",
    "\n",
    "[1] Watkins, C.J.C.H. \"Learning from delayed rewards.\" PhD Thesis, University of Cambridge, England (1989).  \n",
    "[2] Watkins, C.J.C.H., Dayan, P. Q-learning. Mach Learn 8, 279–292 (1992). [Link](https://link.springer.com/article/10.1007/BF00992698)  \n",
    "[3] Thrun, Sebastian, and Anton Schwartz. \"Issues in using function approximation for reinforcement learning.\" Proceedings of the 1993 connectionist models summer school. Psychology Press, 2014. [Link](https://www.taylorfrancis.com/chapters/edit/10.4324/9781315806433-38/issues-using-function-approximation-reinforcement-learning-sebastian-thrun-anton-schwartz)  \n",
    "[4] Hasselt, Hado. \"Double Q-learning.\" Advances in neural information processing systems 23 (2010) [Link](https://papers.nips.cc/paper_files/paper/2010/hash/091d584fced301b442654dd8c23b3fc9-Abstract.html)  \n",
    "[5] Sutton, Richard S., and Andrew G. Barto, \"Temporal Difference Learning\", in *Reinforcement learning: An introduction*.  MIT press, 2018.  \n",
    "[6] Towers, Mark and Terry \"Gymnasium.\" Farama Foundation 2023. [Link](https://gymnasium.farama.org/index.html#)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986901ce-7b77-4fec-9f95-87aa9f6dc3c8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1a998f091ebd01373a3897adf752e21",
     "grade": false,
     "grade_id": "cell-1cca260ea4169208",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Task 2.1 (1 Point)   \n",
    "\n",
    "We will now start with the basics by implementing all relevant variables in `__init__`. All variables used in the algorithm (see above) should be included. In particular, the two Q-functions named `self.q_1` and `self.q_2` should be implemented as NumPy arrays. We initialize all values of the Q-functions with a value of 0.\n",
    "\n",
    "*Hint: Each environment has different quantities and names for its actions and states. To generalise this, states and actions are assigned integers in Gymnasium. The maximum number can be taken from `env.observation_space.n` and `env.action_space.n`*.\n",
    "\n",
    "### Student Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d40459-3ec9-438b-81cc-497bd643d390",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c7f91aeb8d30264f39093229ac36ea0",
     "grade": false,
     "grade_id": "cell-1e67cfc1de63d76c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env, gamma=1.0, learning_rate=0.05, epsilon=0.1):\n",
    "        \"\"\" Initializes the environment and defines dynamics.\n",
    "        \n",
    "        Please DON'T change the names of the variables that are already defined in this method.\n",
    "        The Q-functions shall be called 'self.q_1' and 'self.q_2', besides, there exist 'self.env', 'self.learning_rate', \n",
    "        'self.gamma' and 'self.epsilon'\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba6e896-0541-4c8d-8428-d0edaf4d4fd1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f3a0e64d58ebb141a99c5742f293351e",
     "grade": false,
     "grade_id": "cell-40dca6d03be65695",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede3bf3-324e-4111-8e09-027aa5d87174",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8597b30ac2bd8fe9f4e6c21728b29575",
     "grade": false,
     "grade_id": "cell-c4d396f9a5f257ec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do we have both Q-Functions?\n",
    "\n",
    "map = [\"SFFH\", \"FFFH\", \"HFFH\", \"HFFG\"]\n",
    "test_env = gym.make('CustomFrozenLake-v1', render_mode=None, desc=map, is_slippery=False) \n",
    "test_agent = Agent(test_env, gamma=1.0, learning_rate=0.05, epsilon=0.1)\n",
    "assert test_agent.q_1[0][0] != None\n",
    "assert test_agent.q_2[0][0] != None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4f91a1-111a-476f-adaa-575e87bc69bb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "feacd6447727cf06f5f7b1092cc11fe1",
     "grade": true,
     "grade_id": "cell-909d56604ecc29b6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used for grading. Do not change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62908a45-e929-4c24-88fd-69814b4becfd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09f026617927208d1cc2dceec73fb7c0",
     "grade": false,
     "grade_id": "cell-1ca3b4b185ec39c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Task 2.2 (1 Point)   \n",
    "\n",
    "At this point, we still lack methods to extract actions from the Q-functions of our agent. Implementing these methods effectively creates policies that allow the agent to interact with the environment. Your task is to implement these methods. In particular, we need two different methods:\n",
    "* a greedy method that returns the best possible action given an observation and a Q-function.\n",
    "* an epsilon-greedy method that, given the same input, sometimes returns a random output based on the probability `self.epsilon`.\n",
    "\n",
    "Passing a Q-function is important here, as it allows us to extract values from one or both Q-functions. We already implemented the greedy method below, called `get_best_action`. However, we still miss a method `epsilon_greedy_policy`. Please implement it below, following the description for epsilon-greedy policies as given in the lecture. \n",
    "\n",
    "*Hint: You can use `np.random.random()` for for taking random actions.*\n",
    "\n",
    "*Remark: We chose an implicit definition of the policies; however, one could also use a more explicit definition by creating objects for the policies if desired. Thus, the implementation used in this task is only one of many possible ways to implement the interaction between agent and environment.*\n",
    "\n",
    "### Student Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e956dda-5a90-4b48-a3fe-84a164afd169",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e72974414886c2fcb6d9c9bdc62c91d7",
     "grade": false,
     "grade_id": "cell-8164339c9b68d5ff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_best_action(self, obs, q):\n",
    "    \"\"\" Return the best action based on the Q-function.\n",
    "    \n",
    "    Args: \n",
    "        obs: state of the environment\n",
    "        q: The chosen Q-Function, a numpy array of shape (num_states, num_actions)\n",
    "    Returns:\n",
    "        best_action: Chosen action\n",
    "    \"\"\"\n",
    "    \n",
    "    best_action = np.random.choice(np.flatnonzero(np.isclose(q[obs], (q[obs]).max(), rtol=0.01)))\n",
    "    return best_action\n",
    "\n",
    "def epsilon_greedy_policy(self, obs, q):\n",
    "    \"\"\" Return an action based on the Q-function and probability self.epsilon.\n",
    "    \n",
    "    The action should be random with probability self.epsilon, or otherwise the best action based on the Q-function.\n",
    "    \n",
    "    Args: \n",
    "        obs: state of the environment\n",
    "        q: The chosen Q-Function, a numpy array of shape (num_states, num_actions)\n",
    "    Returns:\n",
    "        action: Chosen action\n",
    "    \"\"\"\n",
    "    \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae0b60e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "74174b7666df401750bf2ac03b773cd1",
     "grade": false,
     "grade_id": "cell-05a50376da9d51cd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "setattr(Agent, 'get_best_action', get_best_action)\n",
    "setattr(Agent, 'epsilon_greedy_policy', epsilon_greedy_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50be68f4-112a-4b80-a50f-2015b7bde228",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "35666ea6bf6cf95a6a601766158da189",
     "grade": false,
     "grade_id": "cell-77d7befebdcdbb7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cba9c8-d117-4a0c-b250-6f6d5efc7bf2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82c0a9a2aa31efb0a9ba7a689d907c5d",
     "grade": false,
     "grade_id": "cell-1546f05a4c57932a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Can we get an action?\n",
    "\n",
    "test_env = gym.make('CustomFrozenLake-v1', render_mode=None, desc=map, is_slippery=False) \n",
    "test_agent = Agent(test_env, epsilon = 0.0)\n",
    "test_agent.q_1[0][0] = 1\n",
    "assert test_agent.epsilon_greedy_policy(0, test_agent.q_1) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328c5f56-fd48-42b9-a574-7e0c51388362",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "680a555b82760f8a76a5aff5265a5779",
     "grade": true,
     "grade_id": "cell-e84d9d0d355495c4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used for grading. Do not change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44301f4-013f-479f-8722-b9e8e9e4efed",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dc86065ddb6217d537d51075a041c48d",
     "grade": false,
     "grade_id": "cell-d158e7e36ad635d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Task 2.3 (1 Point)\n",
    "\n",
    "We can now let the agent and the environment interact with each other. However, we still need to enable our agent to train. To achieve this, we want to implement a method `train` that improves the policy by learning the optimal Q-function. The method only needs the number of episodes to be trained for as input and does not return anything. During a training run, the Q-functions are optimized, thus achieving the optimal policy.\n",
    "\n",
    "We have already provided a rough structure below. Your task is to add the updates for the two Q-functions, as noted in the algorithm above. In the case of success, you should be able to see the Q-function learning about the environment below. \n",
    "\n",
    "*Remark: Since the learning process depends on random variables, it sometimes takes multiple runs of the checkpoint to see improvement in the Q-function.*\n",
    "\n",
    "### Student Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fc9b94-df70-4227-8498-953893fc1d24",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8cdfbcb62b80a8c32b4b746be549908a",
     "grade": false,
     "grade_id": "cell-dfc61e01ae539de8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(self, num_episodes):\n",
    "    \"\"\" Trains the agent with the double-q algorithm.\n",
    "    \n",
    "    Args: \n",
    "        num_episodes: Number of episodes used until training stops\n",
    "    \"\"\"\n",
    "    for i in range(num_episodes+1):\n",
    "        obs, info = self.env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            # In this implementation we only use n=1, but we could extend it for n = ... using a numpy array\n",
    "            # Choose action and perform step\n",
    "            action = self.epsilon_greedy_policy(obs, self.q_1 + self.q_2)\n",
    "            next_obs, reward, done, truncated, info = self.env.step(action)\n",
    "            # TD Update\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "            obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a8d71a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4c0dd43f28a434b081de1395a2b1ab4",
     "grade": false,
     "grade_id": "cell-1e484891eceb980b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "setattr(Agent, 'train', train)\n",
    "setattr(Agent, 'visualize', visualize)\n",
    "setattr(Agent, 'plot_action_value', plot_action_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab62188a-6231-4011-b242-6c6ba09a886a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "315b58dd72fff2670804e7690f1cb349",
     "grade": false,
     "grade_id": "cell-dd5f1266da90cdbf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae24e005-d3b7-477e-81d3-eb2ab139711e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a1c3912c739985703c462aaa6b62b14",
     "grade": false,
     "grade_id": "cell-9bafcccc38fcefea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Does our policy change?\n",
    "\n",
    "map = [\"SFFH\", \"FFFH\", \"HFFH\", \"HFFG\"]\n",
    "test_env = gym.make('CustomFrozenLake-v1', render_mode=None, desc=map, is_slippery=False) \n",
    "test_agent = Agent(test_env)\n",
    "test_agent.visualize(0)\n",
    "test_agent.train(100)\n",
    "test_agent.visualize(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc8d4e5-a1d4-40d6-8ab7-54b737019ba4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ccfbc19d252cdef6d73e80f17012f917",
     "grade": true,
     "grade_id": "cell-6aa7972f09321604",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used for grading. Do not change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7513894c-a244-486b-9a12-230af5c4e18a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b8d89d257665c2b208606177f85fada2",
     "grade": false,
     "grade_id": "cell-185bbddfa146ae12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Task 2.4 (1 Point)\n",
    "\n",
    "Now, our agent can train, but we still lack the method to see the results of our training on the ice.  Thus, your task is to implement a method that exploits the experience we acquired during training. This method shall be called `evaluate`. Here, we want to greedily select our actions according to the sum of both Q-functions to obtain an optimal policy. A rough framework for the evaluation, including rendering, is already given below and only needs to be completed. \n",
    "\n",
    "*Hint: The environment returns a lot of information via `env.step(action)`, some of which might turn out to be unnecessary. For an overview, we refer to the code in the Gymnasium documentation.*\n",
    "\n",
    "### Student Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe3760d-cca0-4013-baaf-b70c44f3a19b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d6d907acd316df63a8c5a868ce16127",
     "grade": false,
     "grade_id": "cell-0f3605469b3176b6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(self, env, file, num_runs=5):\n",
    "    \"\"\" Evaluates the agent in the environment.\n",
    "\n",
    "    Args:\n",
    "        env: Environment we want to use. \n",
    "        file: File used for storing the video.\n",
    "        num_runs: Number of runs displayed\n",
    "    Returns:\n",
    "        done: Info about whether the last run is done.\n",
    "        reward: The reward the agent gathered in the last step.\n",
    "    \"\"\"\n",
    "    frames = []  # collect rgb_image of agent env interaction\n",
    "    video_created = False\n",
    "    for _ in range(num_runs):\n",
    "        done = False\n",
    "        obs, info = env.reset()\n",
    "        out = env.render()\n",
    "        frames.append(out)\n",
    "        while not done:\n",
    "            \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "            out = env.render()\n",
    "            frames.append(out)\n",
    "                \n",
    "    # create animation out of saved frames\n",
    "    if all(frame is not None for frame in frames):\n",
    "        fig = plt.figure(figsize=(10, 6))\n",
    "        plt.axis('off')\n",
    "        img = plt.imshow(frames[0])\n",
    "        def animate(index):\n",
    "            img.set_data(frames[index])\n",
    "            return [img]\n",
    "        anim = FuncAnimation(fig, animate, frames=len(frames), interval=20)\n",
    "        plt.close()\n",
    "        anim.save(file, writer=\"ffmpeg\", fps=5)\n",
    "        \n",
    "    return done, reward\n",
    "\n",
    "setattr(Agent, 'evaluate', evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4318209-6c2f-483f-b0a9-39ea0368394a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1fe7f9a297155859cbeedd2c4b238879",
     "grade": false,
     "grade_id": "cell-3a6f6b6400465589",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83e841f-c4e6-4237-868d-3ccb688b20e3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a281196405b9532905e704346dfeaf79",
     "grade": false,
     "grade_id": "cell-5ba44dfec9b7728e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Can we run evaluate?\n",
    "\n",
    "map = [\"SFFH\", \"FFGH\", \"HFFH\", \"HFFF\"]\n",
    "test_env = gym.make('CustomFrozenLake-v1', render_mode='rgb_array', desc=map, is_slippery=False) \n",
    "test_agent = Agent(test_env)\n",
    "test_agent.q_1[0][2] = 1\n",
    "test_agent.q_1[1][2] = 1\n",
    "test_agent.q_1[2][2] = 1\n",
    "# This policy leads the agent to an ice-hole on the right, for a video check the file \"test_run.mp4\"\n",
    "\n",
    "test_video = \"test_run.mp4\"\n",
    "test_run = test_agent.evaluate(test_env, test_video)[0] \n",
    "assert test_run == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e0e2f2-a557-48b6-b942-b5190e62cbe8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "248cb7f905892b3c6d891baa58da2930",
     "grade": true,
     "grade_id": "cell-4cfcdc3a7d5ab8b1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used for grading. Do not change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f0b0ea-5209-468f-910e-9d0391cd4ae8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4cc5c37496681cd8c027f0364294a3f",
     "grade": false,
     "grade_id": "cell-e2a22988579526c8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Finally: Training the Agent (1 Point)\n",
    "\n",
    "Lastly, we verify that our agent can train in the selected environment. For this purpose, we create a lake and let the agent train for plenty of episodes. If everything has been implemented according to the task, you should see a video of our agent solving the task successfully without drowning. This is also meant to provide feedback on whether your code is working successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38190ca8-de2b-41bb-85bc-c75cdd7a7d33",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67dcfb70591bd9924a1d82f696091056",
     "grade": false,
     "grade_id": "cell-da19f4e22124002a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_runs = 10000\n",
    "map = [\"SFFF\", \"FHFH\", \"FHFH\", \"FFFG\"]\n",
    "env = gym.make('CustomFrozenLake-v1', render_mode='rgb_array', desc=map, is_slippery=False) \n",
    "env.reset()\n",
    "final_agent = Agent(env, gamma=0.9)\n",
    "final_agent.train(training_runs)\n",
    "final_agent.visualize(training_runs)\n",
    "video = \"final_run.mp4\"\n",
    "final_agent.evaluate(env, video, num_runs=1)\n",
    "Video(video, html_attributes=\"loop autoplay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d730026-682f-4db9-98e9-3d564513de2f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "972ee66e4e68b6c871ac78eed1bcabbc",
     "grade": true,
     "grade_id": "cell-ff904bfaebb98ab9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is the end of task 2, please proceed with task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c38ff75-4091-4700-b827-263132c7f538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllbc-bpa",
   "language": "python",
   "name": "rllbc-bpa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
